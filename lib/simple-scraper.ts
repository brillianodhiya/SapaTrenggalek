// Simple scraper without problematic dependencies
export interface ScrapedData {
  content: string;
  source: string;
  source_url: string;
  author: string;
  timestamp: Date;
}

import { TwitterScraper, type TwitterScrapedData } from "./twitter-scraper";
import {
  InstagramScraper,
  type InstagramScrapedData,
} from "./instagram-scraper";

// Keywords untuk monitoring
const MONITORING_KEYWORDS = [
  "trenggalek",
  "pemkab trenggalek",
  "bupati trenggalek",
  "dinas trenggalek",
  "kecamatan trenggalek",
];

export async function scrapeNewsData(): Promise<ScrapedData[]> {
  const results: ScrapedData[] = [];

  try {
    console.log("üîç Starting simple news scraping...");

    // Use native fetch instead of axios to avoid undici issues
    const response = await fetch(
      "https://news.google.com/rss/search?q=trenggalek&hl=id&gl=ID&ceid=ID:id",
      {
        headers: {
          "User-Agent": "Mozilla/5.0 (compatible; SapaTrenggalek/1.0)",
          Accept: "application/rss+xml, application/xml, text/xml",
        },
      }
    );

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const xmlText = await response.text();
    console.log(`üì° Received ${xmlText.length} bytes from Google News`);

    // Simple XML parsing without cheerio
    const items = parseRSSItems(xmlText);
    console.log(`üì∞ Found ${items.length} news items`);

    // Filter items with monitoring keywords
    for (const item of items) {
      const contentLower = (item.title + " " + item.description).toLowerCase();
      const hasKeyword = MONITORING_KEYWORDS.some((keyword) =>
        contentLower.includes(keyword.toLowerCase())
      );

      if (hasKeyword && item.title && item.link) {
        results.push({
          content: `${item.title}\n\n${item.description}`,
          source: "Google News",
          source_url: item.link,
          author: "Various News Sources",
          timestamp: new Date(item.pubDate || Date.now()),
        });
      }
    }

    console.log(`‚úÖ Found ${results.length} relevant news items`);
  } catch (error) {
    console.error("‚ùå Error scraping news:", error);

    // Add fallback sample data if scraping fails
    console.log("üìù Adding fallback sample data...");
    results.push({
      content:
        "Fallback: Pemerintah Kabupaten Trenggalek terus berkomitmen meningkatkan pelayanan publik untuk masyarakat",
      source: "Fallback News",
      source_url: "https://example.com/fallback",
      author: "System Fallback",
      timestamp: new Date(),
    });
  }

  return results;
}

// Simple RSS parser without external dependencies
function parseRSSItems(xmlText: string) {
  const items: Array<{
    title: string;
    link: string;
    description: string;
    pubDate: string;
  }> = [];

  try {
    // Simple regex-based XML parsing (not perfect but works for RSS)
    const itemRegex = /<item[^>]*>([\s\S]*?)<\/item>/gi;
    const titleRegex =
      /<title[^>]*><!\[CDATA\[(.*?)\]\]><\/title>|<title[^>]*>(.*?)<\/title>/i;
    const linkRegex = /<link[^>]*>(.*?)<\/link>/i;
    const descRegex =
      /<description[^>]*><!\[CDATA\[(.*?)\]\]><\/description>|<description[^>]*>(.*?)<\/description>/i;
    const pubDateRegex = /<pubDate[^>]*>(.*?)<\/pubDate>/i;

    let match;
    while ((match = itemRegex.exec(xmlText)) !== null) {
      const itemXml = match[1];

      const titleMatch = titleRegex.exec(itemXml);
      const linkMatch = linkRegex.exec(itemXml);
      const descMatch = descRegex.exec(itemXml);
      const pubDateMatch = pubDateRegex.exec(itemXml);

      const title = (titleMatch?.[1] || titleMatch?.[2] || "").trim();
      const link = (linkMatch?.[1] || "").trim();
      const description = (descMatch?.[1] || descMatch?.[2] || "")
        .replace(/<[^>]*>/g, "")
        .trim();
      const pubDate = (pubDateMatch?.[1] || "").trim();

      if (title && link) {
        items.push({
          title,
          link,
          description,
          pubDate,
        });
      }
    }
  } catch (error) {
    console.error("‚ùå Error parsing RSS:", error);
  }

  return items;
}

// Simulasi scraping media sosial
export async function scrapeSocialMedia(): Promise<ScrapedData[]> {
  return [
    {
      content:
        "Jalan di Kecamatan Panggul rusak parah, mohon segera diperbaiki @pemkabtrenggalek",
      source: "Twitter",
      source_url: "https://twitter.com/example/status/123",
      author: "@warga_trenggalek",
      timestamp: new Date(),
    },
    {
      content:
        "Pelayanan di Disdukcapil Trenggalek sangat memuaskan, terima kasih!",
      source: "Facebook",
      source_url: "https://facebook.com/example/post/456",
      author: "Budi Santoso",
      timestamp: new Date(),
    },
  ];
}

// Convert TwitterScrapedData to ScrapedData format
function convertTwitterData(twitterData: TwitterScrapedData[]): ScrapedData[] {
  return twitterData.map((tweet) => ({
    content: tweet.content,
    source: tweet.source,
    source_url: tweet.source_url,
    author: tweet.author,
    timestamp: tweet.timestamp,
  }));
}

// Convert InstagramScrapedData to ScrapedData format
function convertInstagramData(
  instagramData: InstagramScrapedData[]
): ScrapedData[] {
  return instagramData.map((post) => ({
    content: post.content,
    source: post.source,
    source_url: post.source_url,
    author: post.author,
    timestamp: post.timestamp,
  }));
}

export async function scrapeTwitterData(): Promise<ScrapedData[]> {
  try {
    console.log("üê¶ Starting Twitter scraping...");

    // Check if Twitter API is configured
    const isConfigured = !!(
      process.env.TWITTER_API_KEY &&
      process.env.TWITTER_API_SECRET &&
      process.env.TWITTER_ACCESS_TOKEN &&
      process.env.TWITTER_ACCESS_TOKEN_SECRET
    );

    if (!isConfigured) {
      console.log("‚ö†Ô∏è Twitter API not configured, using fallback data...");
      return getTwitterFallbackData();
    }

    const twitterScraper = new TwitterScraper();

    // Scrape recent tweets about Trenggalek
    const recentTweets = await twitterScraper.scrapeRecentTweets(30);

    // Also scrape from official accounts if configured
    const officialAccounts = [
      "pemkabtrenggalek", // Official Pemkab account (if exists)
      "humas_trenggalek", // Official Humas account (if exists)
    ];

    const timelineTweets: TwitterScrapedData[] = [];
    for (const account of officialAccounts) {
      try {
        const tweets = await twitterScraper.scrapeUserTimeline(account, 10);
        timelineTweets.push(...tweets);
      } catch (error) {
        console.log(`‚ö†Ô∏è Could not scrape @${account}, might not exist`);
      }
    }

    const allTwitterData = [...recentTweets, ...timelineTweets];
    console.log(
      `‚úÖ Twitter scraping completed: ${allTwitterData.length} tweets`
    );

    // If no data from API, use fallback
    if (allTwitterData.length === 0) {
      console.log("üìù No Twitter data found, using fallback...");
      return getTwitterFallbackData();
    }

    return convertTwitterData(allTwitterData);
  } catch (error) {
    console.error("‚ùå Twitter scraping failed:", error);
    console.log("üìù Using Twitter fallback data...");
    return getTwitterFallbackData();
  }
}

function getTwitterFallbackData(): ScrapedData[] {
  return [
    {
      content:
        "Pelayanan di Kantor Kecamatan Trenggalek sangat memuaskan hari ini. Terima kasih kepada petugas yang ramah dan profesional! #PelayananPublik #Trenggalek",
      source: "Twitter/X",
      source_url: "https://twitter.com/example/status/123456789",
      author: "@warga_trenggalek",
      timestamp: new Date(Date.now() - 2 * 60 * 60 * 1000), // 2 hours ago
    },
    {
      content:
        "Jalan di Kecamatan Panggul perlu perbaikan segera. Banyak lubang yang membahayakan pengendara. Mohon perhatian @pemkabtrenggalek",
      source: "Twitter/X",
      source_url: "https://twitter.com/example/status/123456790",
      author: "@panggul_update",
      timestamp: new Date(Date.now() - 4 * 60 * 60 * 1000), // 4 hours ago
    },
    {
      content:
        "Selamat kepada Tim Sepak Bola Trenggalek yang berhasil juara di turnamen antar kabupaten! Bangga dengan prestasi anak daerah üèÜ #TrenggalekJuara",
      source: "Twitter/X",
      source_url: "https://twitter.com/example/status/123456791",
      author: "@sports_trenggalek",
      timestamp: new Date(Date.now() - 6 * 60 * 60 * 1000), // 6 hours ago
    },
    {
      content:
        "Pasar Tradisional Trenggalek semakin ramai setelah renovasi. Pedagang dan pembeli sama-sama senang dengan fasilitas baru yang lebih nyaman.",
      source: "Twitter/X",
      source_url: "https://twitter.com/example/status/123456792",
      author: "@pasar_trenggalek",
      timestamp: new Date(Date.now() - 8 * 60 * 60 * 1000), // 8 hours ago
    },
    {
      content:
        "Program vaksinasi COVID-19 di Puskesmas Trenggalek berjalan lancar. Masyarakat antusias mengikuti program pemerintah untuk kesehatan bersama.",
      source: "Twitter/X",
      source_url: "https://twitter.com/example/status/123456793",
      author: "@kesehatan_trenggalek",
      timestamp: new Date(Date.now() - 12 * 60 * 60 * 1000), // 12 hours ago
    },
  ];
}

export async function scrapeInstagramData(): Promise<ScrapedData[]> {
  try {
    console.log("üì∏ Starting Instagram scraping...");

    const instagramScraper = new InstagramScraper();

    // Official Instagram accounts to scrape
    const officialAccounts = [
      "pemkabtrenggalek", // Official Pemkab account
      "humas_trenggalek", // Official Humas account
      "dispar_trenggalek", // Tourism department
      "trenggalekhits", // Popular local account
      "exploretrenggalek", // Tourism/exploration account
      "kuliner_trenggalek", // Food/culinary account
    ];

    // Use the new scrapeAll method for comprehensive scraping
    const allInstagramData = await instagramScraper.scrapeAll();

    console.log(
      `‚úÖ Instagram scraping completed: ${allInstagramData.length} posts`
    );

    return convertInstagramData(allInstagramData);
  } catch (error) {
    console.error("‚ùå Instagram scraping failed:", error);
    return [];
  }
}

export async function scrapeAllSources(): Promise<ScrapedData[]> {
  console.log("üöÄ Starting comprehensive scraping from all sources...");

  const [newsData, socialData, twitterData, instagramData] = await Promise.all([
    scrapeNewsData(),
    scrapeSocialMedia(),
    scrapeTwitterData(),
    scrapeInstagramData(),
  ]);

  const totalResults =
    newsData.length +
    socialData.length +
    twitterData.length +
    instagramData.length;
  console.log(`üìä Scraping summary:
    - News: ${newsData.length} items
    - Social Media: ${socialData.length} items  
    - Twitter: ${twitterData.length} items
    - Instagram: ${instagramData.length} items
    - Total: ${totalResults} items`);

  return [...newsData, ...socialData, ...twitterData, ...instagramData];
}
